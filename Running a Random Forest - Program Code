# -*- coding: utf-8 -*-
"""
Created on Thu Mar  3 10:51:10 2016
@author: Bernard
"""

from pandas import Series, DataFrame
import pandas as pd
import numpy as np
import os
import matplotlib.pylab as plt
from sklearn.cross_validation import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report
import sklearn.metrics
 # Feature Importance
from sklearn import datasets
from sklearn.ensemble import ExtraTreesClassifier

#os.chdir("C:\TREES")
os.chdir("C:\TREES\Random Forest")


AH_data = pd.read_csv('gapminder_ghana_updated.csv')

# convert to numeric format
AH_data['incomeperperson'] = pd.to_numeric(AH_data['incomeperperson'], errors='coerce')
AH_data['lifeexpectancy'] = pd.to_numeric(AH_data['lifeexpectancy'], errors='coerce')
AH_data['exports'] = pd.to_numeric(AH_data['exports'], errors='coerce')
AH_data['inflation'] = pd.to_numeric(AH_data['inflation'], errors='coerce')
AH_data['agriculture'] = pd.to_numeric(AH_data['agriculture'], errors='coerce')
AH_data['democracyscore'] = pd.to_numeric(AH_data['democracyscore'], errors='coerce')

#drop all NaN values
data_clean = AH_data.dropna()



#convert quantitative responsive variable to binary categorical variable
def lifeExpectancyCat (row):
    if row["lifeexpectancy"]<=60:
        return 0
    elif row["lifeexpectancy"] >60:
        return 1   


data_clean["lifeExpectancyCat"] = data_clean.apply(lambda row: lifeExpectancyCat(row), axis =1)

#creating categorical explanatory variables out of exports
def exportsCatGrp (row):
    if row["exports"] <=40:
        return 0
    elif row["exports"] >40:
        return 1
    
        
data_clean["exportsCatGrp"] = data_clean.apply(lambda row:exportsCatGrp(row), axis =1)


#creating a categorical explanatry variable out of incomeperperson
def incomeLevelGrp (row):
    if row["incomeperperson"] <=2200:
        return 0
    elif row["incomeperperson"] >2200:
        return 1
        
data_clean["incomeLevelGrp"] = data_clean.apply(lambda row:incomeLevelGrp(row), axis =1)


#data_clean = AH_data
data_clean.dtypes

data_clean.describe()


"""
Modeling and Prediction
"""
#Split into training and testing sets
predictors = data_clean[['incomeLevelGrp','exportsCatGrp','inflation','agriculture','democracyscore']]


targets = data_clean.lifeExpectancyCat

pred_train, pred_test, tar_train, tar_test  = train_test_split(predictors, targets, test_size=.4)

pred_train.shape
pred_test.shape
tar_train.shape
tar_test.shape

#Build model on training data
from sklearn.ensemble import RandomForestClassifier

classifier=RandomForestClassifier(n_estimators=25)
classifier=classifier.fit(pred_train,tar_train)

predictions=classifier.predict(pred_test)

sklearn.metrics.confusion_matrix(tar_test,predictions)
sklearn.metrics.accuracy_score(tar_test, predictions)


# fit an Extra Trees model to the data
model = ExtraTreesClassifier()
model.fit(pred_train,tar_train)
# display the relative importance of each attribute
print("display the relative importance of each attribute")
print(model.feature_importances_)


"""
Running a different number of trees and see the effect
 of that on the accuracy of the prediction
"""

trees=range(25)
accuracy=np.zeros(25)

for idx in range(len(trees)):
   classifier=RandomForestClassifier(n_estimators=idx + 1)
   classifier=classifier.fit(pred_train,tar_train)
   predictions=classifier.predict(pred_test)
   accuracy[idx]=sklearn.metrics.accuracy_score(tar_test, predictions)
   
plt.cla()
plt.plot(trees, accuracy)
